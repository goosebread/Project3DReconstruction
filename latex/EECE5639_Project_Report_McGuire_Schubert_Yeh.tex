\documentclass[12pt,letterpaper]{article} % twocolumn

\usepackage[left=0.8in, right=0.8in, bottom=1in, top=1in]{geometry}
\usepackage{expl3}
\usepackage[dvipsnames,table]{xcolor} % table arg for coloring table cells
\usepackage{titling}  % Customizing \maketitle

\RequirePackage{hyperref}
% Color hyperlinks dark blue by default.
\hypersetup{
    colorlinks=true,
    linkcolor=MidnightBlue,
    citecolor=MidnightBlue,
    urlcolor=MidnightBlue,
}

% Set font.
\usepackage{newtxmath}
\usepackage{fontspec}
\setmainfont{Times New Roman}
\setsansfont{Latin Modern Sans}
\setmonofont{Inconsolata}
% Bibliography.
\usepackage[backend=bibtex,style=ieee]{biblatex} % backend=bibtex
\addbibresource{EECE5639_Project_Report_McGuire_Schubert_Yeh.bib}

\usepackage{lipsum}
\usepackage[pagestyles]{titlesec}
\titlespacing{\section}
{0pc}{8pt}{4pt}[0pc] % left before after right

\titleformat{\section}[block]
{\centering\large\bfseries}
{\thesection.}
{1ex minus .1ex}
{\MakeUppercase}

\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tweak \maketile layout
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setlength{\droptitle}{-0.75in}

\DeclareDocumentCommand{\FancyEmail}{ m }{%
    \colorlet{@savecolor}{.}%
    \href{mailto:#1}{\color{@savecolor}\ttfamily{}#1}%
}
\title{\bfseries{} Quantitative Evaluation of a 3D Reconstruction Algorithm}
\author{
%    \itshape{}
    Nathan McGuire \\ \FancyEmail{mcguire.n@northeastern.edu} \and
%    \itshape{}
    Brian Schubert \\ \FancyEmail{schubert.b@northeastern.edu}\and
%    \itshape{}
    Alex Yeh \\ \FancyEmail{yeh.al@northeastern.edu}
}
\date{April 24th, 2024}

\renewcommand{\baselinestretch}{1.5}
\begin{document}
\maketitle
\vfill
\begin{abstract}
    We implement an end-to-end 3D reconstruction system and a suite of tests for quantifying its performance. The reconstruction system ingests a series of 2D images of an object and generates a representative mesh of the object in 3D space. We follow the development in \cite{9229407} to design this system. The core algorithm performs feature point extraction and bundle adjustment to estimate 3D point clouds, from which the final mesh is reconstructed.  Furthermore, we have created a test system to generate synthetic images from 3D models.  The reconstructed meshes can be compared against the initial meshes to determine overall system performance.
\end{abstract}
\clearpage
\tableofcontents
\clearpage



\section{Introduction}
3D reconstruction is a procedure where many 2D images of a single object, taken from different poses, are used to determine the 3D geometry of the object.  Measuring the 3D geometry of objects is widely applicable, with applications ranging from planetary exploration, self-driving cars, virtual reality, and medical devices.  Using cameras for this task, instead of a sensor that can directly measure the 3D geometry, is advantageous because of the low cost of cameras and the fact that they are already widely deployed on existing systems.

It is not possible to directly compute 3D geometry from a single camera image.  Cameras measure the intensity and direction of incoming light, but not the distance to the source.  As such, at least two camera images of the same point are needed to calculate the 3D position of that point.  A simple 3D reconstruction pipeline has 3 major steps: feature extraction, feature matching, and triangulation.  Feature extraction attempts to find a large number of distinctive points in each image, feature matching attempts to pair up these points across multiple images, and triangulation uses these matches, a model of how light enters the camera, and a model of where the images were taken from to solve for the position of the points.  If the camera poses are not known, a sufficiently large number of point matches also enables the relative camera pose to be determined.  In this case, because there is no prior information about the geometry of the object or camera poses, it is not possible to determine the scale of the object, only the relative geometry of the object and camera.

A variety of factors limit the performance of real-world reconstruction systems.  Because triangulation of feature pairs relies on a model of what direction in 3D space a given pixel in image space maps to, inaccuracies in the camera model directly propagate into inaccuracies in the position of reconstructed points.  The camera model is typically determined empirically for an individual camera, since tiny variations caused by assembly can have significant impacts.  Furthermore, even a well calibrated camera will experience image noise, as camera sensors are electronic devices and are susceptible to a variety of noise sources.  This results in inaccurate pixel intensity values, which results in less feature locations.  Furthermore, since the magnitude of the noise is not constant between successive images, image noise results in differences in the feature description of a given feature in successive images, and thus less accurate feature matching.

\section{Methods}
We chose to use synthetic data to evaluate our reconstruction algorithm. This provides us with a known ground-truth mesh to compare against, and allows us to avoid pitfalls inherent in using real data, like poorly calibrated cameras.  Our image generation system is built around the pyrender library \cite{pyrender}, which is itself a python wrapper around OpenGL.  OpenGL is a widely used graphics library which handles rendering of 2D and 3D scenes.  It allows rendering of textured surfaces with a variety of realistic lighting and material effects.  Our image data generation system renders a scene constructed from a library of 3D models we’ve put together as test elements.  Individual models we used include pyrender example models of a drill and soda bottle, as well as freely accessible 3d models of coral of varying fidelity.  Rendering known 3D models ensures that the images are generated in a repeatable, consistent manner, and allows us to control for variables like the camera intrinsic parameters.  We can tune the complexity of the reconstruction by adding more or more complex 3D models to the scene and we can quantify the robustness of the reconstruction algorithm by manually adding noise to the generated images.  Generating images this way also gives us direct access to the 3D geometry used to create the images, which is crucial for evaluating the accuracy of the resulting reconstruction.

Feature extraction is performed via the OpponentSIFT algorithm \cite{5204091}.  SIFT is a feature extractor that detects distinct points using the difference of gaussians across a range of image scales, then describes the feature using the orientation of the local gradient around that point.  Because SIFT searches through the scale-space and pixel space it is invariant to feature scale changes across images.  This is useful for reconstruction, as camera motion is likely to cause changes in object scale.  Opponent SIFT is an extension to SIFT that transforms the image from a RGB colorspace to an opponent color space. The opponent color space uses three dimensions (white-black, red-green, yellow-blue). Intensity is encoded directly into one channel while the other two channels encode only color. This makes the resulting feature detections more robust with respect to illumination changes compared to traditional SIFT methods. Traditional SIFT descriptors are calculated for each opponent color space channel and flattened to form one large descriptor for each feature point.

Feature Matching is performed using brute force matching between pairs of images. For every feature in one image, the best and second best matching features in the second image are found, ranked by the L2 distance between the descriptor vectors. A ratio test is applied to filter out possibly ambiguous matches by only keeping matches if the distance to the best match divided by the distance to the second match is less than 0.8.  Outliers are further filtered using a RANSAC method on all matches between two images. OpenCV internally uses a 7-point algorithm to estimate a fundamental matrix for each RANSAC sample, and points matches are evaluated on how well they fit the epipolar geometry.

For the remaining point matches, the algebraic solution is used to recover 3D points. For a pair of features in two images, the coordinates of the features in image space are transformed into rays in camera coordinates using the known camera intrinsics. Due to the discrete nature of pixels, the rays do not intersect perfectly. Therefore, these rays are all converted into the first camera’s coordinate frame and the direction of the skew segment between these rays is calculated using their cross product. A linear system of equations is set up to calculate the lengths of the two rays and skew segment. As a final way to filter out poorly matched points, if the skew segment length is too large, the point is ignored. This is evaluated by comparing the skew segment length divided by the L2 norm of the three lengths against an empirically determined threshold of $\num{1e-2}$. If the skew segment length is not too large, the midpoint of the skew segment is taken as the triangulated point and converted into the world coordinate frame.  This process is repeated for every feature match to create the point cloud.

Once we have a point cloud, we need to reconstruct a mesh and compare against the ground truth.  A mesh is parameterized by a list of points and a list of triangular faces spanning those points.  Furthermore, the mesh needs to be closed so that we can perform comparisons against the ground truth mesh.  Though a mesh with holes is useful for a variety of reconstruction applications, the comparisons we have chosen as the baseline require the ability to compute the volume of the mesh.  There are a variety of approaches to solve this, and the current best that we have tested is Poisson Surface Reconstruction \cite{10.5555/1281957.1281965}.  This algorithm attempts to solve an optimization problem to create a smooth surface that minimizes the deviation from the point cloud.  Importantly, all the object reconstruction algorithms we tested assume that the entire point cloud belongs to a single object. As we might be imaging multiple objects, we implemented a point cloud clustering algorithm to segment the cloud by selecting continuous, dense areas.  Once we have reconstructed a mesh for an object, we compute the ratio between the intersection volume and union volume of the reconstructed and ground truth meshes.

%
%\nocite{9229407}
%\nocite{DVN/CU4UXG_2021}
%\nocite{Olsson}
%\nocite{shrestha2022real}

\clearpage
%\section{References}
\printbibliography%[heading=none]

\end{document}
